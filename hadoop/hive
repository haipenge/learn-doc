1.Build Hive for hadoop 
  git clone https://git-wip-up.apache.org/repos/asf/hive.git
  cd hive
  mvn clean package -Pdist
  Note:build hive support hadoop 2.x
  mvn clean package -Phadoop-2,dist

2.配置Hive
 $ $HADOOP_HOME/bin/hadoop fs -mkdir       /tmp
 $ $HADOOP_HOME/bin/hadoop fs -mkdir -p       /user/hive/warehouse
 $ $HADOOP_HOME/bin/hadoop fs -chmod g+w   /tmp
 $ $HADOOP_HOME/bin/hadoop fs -chmod g+w   /user/hive/warehouse

 如果要使用不同的hive仓库目录
 保持以上目录与hive-default.xml hive.metastore.warehouse.dir 一致

 3.Run Hive 
   1).Run hive cli:
     $HIVE_HOME/bin/hive
   2).Run HiveServer2 and Beeline
     $HIVE_HOME/hive --service metastore
    <db type> derby
    $HIVE_HOME/bin/schematool -dbType <db type> -initSchema
    Exam:
    $HIVE_HOME/bin/schematool -dbType derby -initSchema
    $HIVE_HOME/bin/schematoll -dbType mysql -info

    schematool -dbType mysql -initSchema

    $HIVE_HOME/bin/schematool -dbType mysql -initSchema


    3).Run HieveServer1 and beeline form shell:
      $HIVE_HOME/bin/hiveserver2  
      $HIVE_HOME/bin/beeline -u jdbc:hive2://$HS2_HOST:$HS2_PORT

         Default Jdbc url for hiveserver2 is:
            jdbc:hive2://localhost:10000
         Exam:$HIVE_HOME/bin/beeline -u jdbc:hive2://
    4).Run HCatalog
       $HIVE_HOME/hcatalog/sbin/hcat_server.sh
       Use:$HIVE_HOME/hcatalog/bin/hcat

       Run WebHCat
         $HIVE_HOME/hcatalog/sbin/webhcat_server.sh
 4.Runtime Config
    使用map reduce进行数据处理
    beeline>SET  mapred.job.tracker=localhost:50030;
    beeline>SET -v;
 5.Create table
    hive>create table invites(foo INT,bar STRING) PARTITIONED BY (ds STRING);
 6.Show tables
    hive>show tables;
    hive>show tables '.*s';

$HIVE_HOME/bin/schematool -dbType derby -initSchema

1).Start hive as service

hive --service metastore &

2).datanucleus.schema.autoCreateAll
 at fist time start hive and sqoop,
 set datanucleus.schema.autoCreateAll to 'true' at hive-default.xml

3).fix :'Could not register mbeans java.security.AccessControlException: access denied'
 add to $JAVA_HOME/jre/lib/security/java.security

 permission java.util.PropertyPermission "*", "read,write";

 7.启动 hiveserver for jdbc
 hive --service hiveserver2 &


二.Hive With HBase

cd $HIVE_HOME
cp $HBASE_HOME/lib/hbase-common-*.jar lib/aux-jars/
cp $HBASE_HOME/lib/hbase-client-*.jar lib/aux-jars/
cp $HBASE_HOME/lib/hbase-server-*.jar lib/aux-jars/
cp $HBASE_HOME/lib/hbase-hadoop2-compat-*.jar lib/aux-jars/
cp $HBASE_HOME/lib/netty-all-*.Final.jar lib/aux-jars/
cp $HBASE_HOME/lib/hbase-protocol-*.jar lib/aux-jars/
cp $HBASE_HOME/lib/zookeeper-*.jar lib/aux-jars/

exam:

1).创建普通表,hbase_hive_1 ->xyz
CREATE TABLE hbase_hive_1(key int, value string)   
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'   
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf1:val")   
TBLPROPERTIES ("hbase.table.name" = "xyz"); 

2).创建带分区的表
配置修改：创建动态分区，需设置  hive.exec.dynamic.partition:true
Exam:
  hive> set hive.exec.dynamic.partition;
  hive.exec.dynamic.partition=false
  hive> set hive.exec.dynamic.partition=true;
  hive> set hive.exec.dynamic.partition;
  hive.exec.dynamic.partition=true

CREATE TABLE hbase_hive_2(key int, value string)    
partitioned by (day string)   
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'   
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf1:val")   
TBLPROPERTIES ("hbase.table.name" = "xyz2");

在已存在的表上创建分区：
添加分区表语法（表已创建，在此基础上添加分区）：ALTER TABLE table_name ADD
partition_spec [ LOCATION 'location1' ]
partition_spec [ LOCATION 'location2' ] ...

ALTER TABLE day_table ADD
PARTITION (dt='2008-08-08', hour='08')
location '/path/pv1.txt'

ALTER TABLE table_name ADD PARTITION (partCol = 'value1') location 'loc1'; //示例
ALTER TABLE table_name ADD IF NOT EXISTS PARTITION (dt='20130101') LOCATION '/user/hadoop/warehouse/table_name/dt=20130101'; //一次添加一个分区

ALTER TABLE page_view ADD PARTITION (dt='2008-08-08', country='us') location '/path/to/us/part080808' PARTITION (dt='2008-08-09', country='us') location '/path/to/us/part080809';  //一次添加多个分区

删除分区：
ALTER TABLE table_name DROP
partition_spec, partition_spec,...
用户可以用 ALTER TABLE DROP PARTITION 来删除分区。分区的元数据和数据将被一并删除。例：
ALTER TABLE day_hour_table DROP PARTITION (dt='2008-08-08', hour='09');

基于分区的查询
SELECT day_table.* FROM day_table WHERE day_table.dt>= '2008-08-08';

查看分区语句：
hive> show partitions day_hour_table; 
OK dt=2008-08-08/hour=08 dt=2008-08-08/hour=09 dt=2008-08-09/hour=09


3).创建关联表：
create external table example 
(rowkey string,
basic map<string,string>,
other map<string,string>)   
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'   
WITH SERDEPROPERTIES ("hbase.columns.mapping" =":key,basic:,other:")    
TBLPROPERTIES  ("hbase.table.name" = "example"); 


4).制表符进行分割 
CREATE TABLE t2(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;

5).分区表命令 
CREATE TABLE t3(id int) PARTITIONED BY (day int); 
LOAD DATA LOCAL INPATH ‘/root/id’ INTO TABLE t1 PARTITION (day=22);

6).桶表命令 
create table t4(id int) clustered by(id) into 4 buckets; 
set hive.enforce.bucketing = true; 
使用桶加载数据 不能使用load data方式加载数据 
insert into table t4 select id from t3;

exam:
 create table wyp
    > (id int, name string,
    > age int, tel string)
    > ROW FORMAT DELIMITED
    > FIELDS TERMINATED BY '\t'
    > STORED AS TEXTFILE;

注：一般不使用textfile,
textfile、sequencefile、rcfile等。用户也可以通过实现接口来自定义输入输的文件格式。
在实际应用中，textfile由于无压缩，磁盘及解析的开销都很大，一般很少使用。Sequencefile以键值对的形式存储的二进制的格式，其支持针对记录级别和块级别的压缩。rcfile是一种行列结合的存储方式（text file和sequencefile都是行表[row table]），其保证同一条记录在同一个hdfs块中，块以列式存储

导入数据：
1).从文件导入数据
  load data local inpath 'wyp.txt' into table wyp;
2).从HDFS上导入数据
  load data inpath '/home/wyp/add.txt' into table wyp;
3).将其它表的查询结果插入新表中
  insert into table test
    > partition (age='25')
    > select id, name, tel
    > from wyp;
4).多表插入
  hive> from wyp
    > insert into table test
    > partition(age)
    > select id, name, tel, age
    > insert into table test3
    > select id, name
    > where age>25;
5).创建表的同时，插入数据
   hive> create table test4
    > as
    > select id, name, tel
    > from wyp;

提高性能：
#将小文件合并为大文件 
setmapred.max.split.size=256000000  #每个Map最大输入大小（单位：字节）
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat  #执行Map前进行小文件合并

set hive.merge.mapfiles= true                 #在Map-only的任务结束时合并小文件  
sethive.merge.mapredfiles= true               #在Map-Reduce的任务结束时合并小文件  
set hive.merge.size.per.task= 256*1000*1000    #合并文件的大小  
set hive.merge.smallfiles.avgsize=16000000     #当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge 

